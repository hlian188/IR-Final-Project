{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "#import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset_tweets_WHO.txt'\n",
    "\n",
    "#convert the text to json\n",
    "with open(path) as f:\n",
    "    tweets_json = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(line):\n",
    "    \"\"\"\n",
    "    Helper function to remove punctuation\n",
    "    \n",
    "    Arugment:\n",
    "    line -- string of text\n",
    "    \n",
    "    Returns:\n",
    "    line -- string of text without punctuation\n",
    "    \"\"\"\n",
    "    return line.translate(str.maketrans('', '', string.punctuation.replace('#', '')))\n",
    "\n",
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) by removing stop words, emojis, and punctuation and\n",
    "    stemming, transforming to lowercase and returning the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line -- a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line =  line.lower() ## Transform in lowercase\n",
    "    line = \"\".join(c for c in line if c in string.printable) ## remove non-ASCII terms like emojis and symbols\n",
    "    line = remove_punct(line) ## remove punctuation\n",
    "    line = line.split() ## Tokenize the text to get a list of terms\n",
    "    \n",
    "    line = [word for word in line if not (re.match(\"^qampa$\" , word) or re.match(\"^amp$\" , word) or re.match(\"^http\" , word)) \n",
    "    and word] #remove html tags, blank spaces like '', and urls\n",
    "    line = [word for word in line if not word.isnumeric()]\n",
    "    \n",
    "    line = [word for word in line if word not in stop_words] ##eliminate the stopwords\n",
    "    line = [stemmer.stem(word) for word in line] ## perform stemming\n",
    "    \n",
    "    # add unhashtagged word if it's hashtag is present \n",
    "    # e.x. if #covid is present, we also add covid as a token\n",
    "    line = line + [word.replace('#', '') for word in line if word[0] == '#' ] \n",
    "    \n",
    "    line = list(set(line)) #remove duplicates\n",
    "    \"\"\"to discuss, should we remove hashtags? \n",
    "    yes because if we don't, then something e.x. #covid and covid will have different term frequencies. A document that only\n",
    "    contains '#covid' is probably very relevant for covid, but will get a tf of 0 if we are searching using the query 'covid'\n",
    "    \"\"\"\n",
    "    \n",
    "    #should we remove numbers? also ellipses are still present\n",
    "    #also need to lemmatize words\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tweets_json:\n",
    "    key = str(key)\n",
    "    tweets_json[key]['full_text'] = build_terms(tweets_json[key]['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openwho', 'intern', 'respons', 'equip', 'work', 'today', 'learn', 'ready4respons', 'launch', 'public', 'risk', 'curriculum', 'core', 'within', 'disast', '#openwho', 'multiti', '#ready4respons', 'need', 'day', 'reduct', 'compet', 'health', 'emerg', 'start', 'help']\n",
      "['commun', 'weak', 'popul', 'live', 'like', 'especi', 'shown', 'entir', 'vulner', 'migrant', 'indigen', 'condit', 'system', 'disast', 'peopl', 'fragil', 'covid19', '#covid19', 'humanitarian', 'health', 'emerg', 'affect']\n",
      "['strong', 'group', 'intern', 'respond', 'invest', 'equit', 'readi', 'system', 'better', 'risk', 'countri', 'care', 'resili', 'disast', 'must', 'marginalis', 'access', 'day', 'protect', 'achiev', 'reduct', 'health', 'emerg', 'suppli', 'gender', 'ensur', 'equiti']\n",
      "['congratul', 'reach', '#algeria', '16th', 'africa', 'whoafro', 'rt', '#africa', 'pop', 'fulli', 'mileston', 'countri', 'vaccin', 'algeria']\n",
      "['esperando', 'completament', 'todava', 'covid19', 'vacunado', 'pued', 'contraer', 'rt', 'opsom', 'importa', 'est', 'si']\n",
      "['checkbeforeyoushar', 'whosearo', 'verifi', 'share', 'rt', 'inform', 'come', 'sourc', '#checkbeforeyoushar', '#inform']\n",
      "['appreci', 'role', 'develop', 'covid19', '#covid19', 'drtedro', 'rt', 'respons', 'sector', 'must', 'short', 'play', 'vaccin', 'privat']\n",
      "['miser', 'injustic', 'intellectu', 'whether', 'fail', 'drtedro', 'rt', 'waiv', 'properti', 'right', 'debat', 'human', 'vaccin']\n",
      "['impact', 'peopl', 'covid19', '#covid19', 'doctorja', 'major', 'mentalhealth', 'drtedro', '#mentalhealth', 'rt', 'whophilippin', 'dedic', 'work', 'thank']\n",
      "['immor', 'peopl', '#healthwork', 'broad', 'drtedro', 'time', 'rt', 'administr', 'mani', 'healthwork', 'unfair', 'risk', 'unjust', 'booster', 'dose']\n",
      "['2004a', 'covid19', '#covid', 'covid', 'goos', 'gees']\n"
     ]
    }
   ],
   "source": [
    "#inspect the first couple of tweets to check\n",
    "for i in range(10):\n",
    "    print(tweets_json[str(i)]['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
