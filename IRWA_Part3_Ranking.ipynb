{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "**LAB 3: Ranking**\n",
        "\n",
        "Harrison Lian U196989\n",
        "\n",
        "Hugo Da Silva U191838\n",
        "\n",
        "Brayan GonzÃ¡lez U172820"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**IMPORTS**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from collections import defaultdict\n",
        "from array import array\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "import numpy as np\n",
        "import collections\n",
        "from numpy import linalg as la\n",
        "import json\n",
        "import re\n",
        "import string"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**AUX FUNCTIONS**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def pretty_print_tweet(tweet):\n",
        "    print(\n",
        "    \"\"\"id: {}\n",
        "    username: {}\n",
        "    text: {}\n",
        "    date: {}\n",
        "    hashtags: {}\n",
        "    likes: {}\n",
        "    retweets: {}\n",
        "    url: {}\\n\"\"\".format(tweet['id'], tweet['username'], tweet['full_text'], tweet['date'], tweet['hashtags'], tweet['likes'],\n",
        "                        tweet['retweets'], tweet['url']))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# **LOAD DATA AND CLEAN UP**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "path = 'dataset_tweets_WHO.txt'\n",
        "\n",
        "#convert the text to json\n",
        "with open(path) as f:\n",
        "    tweets_json = json.load(f)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "print(json.dumps(tweets_json['50'], indent=4, sort_keys=True))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def remove_punct(line):\n",
        "    \"\"\"\n",
        "    Helper function to remove punctuation EXCEPT for '#''\n",
        "    \n",
        "    Arugment:\n",
        "    line -- string of text\n",
        "    \n",
        "    Returns:\n",
        "    line -- string of text without punctuation\n",
        "    \"\"\"\n",
        "    return line.translate(str.maketrans('', '', string.punctuation.replace('#', '')))\n",
        "\n",
        "def build_terms(line):\n",
        "    \"\"\"\n",
        "    Preprocess the Tweet text by removing stop words, emojis, and punctuation and\n",
        "    stemming, transforming to lowercase and returning the tokens of the text.\n",
        "    \n",
        "    Argument:\n",
        "    line -- string (text) to be preprocessed\n",
        "    \n",
        "    Returns:\n",
        "    line -- a list of tokens corresponding to the input text after the preprocessing\n",
        "    \"\"\"\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    # transform to lowercase \n",
        "    line =  line.lower() \n",
        "    \n",
        "    # remove non-ASCII terms like emojis and symbols\n",
        "    line = \"\".join(c for c in line if c in string.printable) \n",
        "    \n",
        "    # remove punctuation EXCEPT for hashtags (see remove_punct())\n",
        "    line = remove_punct(line)\n",
        "    \n",
        "    # tokenize the text to get a list of terms\n",
        "    line = line.split() \n",
        "    \n",
        "    # remove html tags, blank spaces like '', and urls\n",
        "    line = [word for word in line if not (re.match(\"^qampa$\" , word) or re.match(\"^amp$\" , word) or re.match(\"^http\" , word)) \n",
        "    and word] \n",
        "    \n",
        "    # remove standalone numbers e.x. '19' but not the 19 from 'covid19'\n",
        "    line = [word for word in line if not word.isnumeric()]\n",
        "    \n",
        "    # add standalone word as token too if it has number e.x. 'covid19' gets tokenized as 'covid19' and 'covid'\n",
        "    line = line + [word.rstrip(string.digits) for word in line if sum([c.isdigit() for c in word]) != 0]\n",
        "    \n",
        "    # remove stopwords\n",
        "    line = [word for word in line if word not in stop_words] \n",
        "    \n",
        "    # perform stemming\n",
        "    line = [stemmer.stem(word) for word in line]\n",
        "    \n",
        "    # add unhashtagged word if it's hashtag is present \n",
        "    # e.x. if #covid is present, we also add covid as a token\n",
        "    line = line + [word.replace('#', '') for word in line if word[0] == '#' ] \n",
        "    \n",
        "    return line"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# tweet_dict is our output data structure that maps Tweet IDs to their text\n",
        "# note we need to keep the following information\n",
        "# Tweet | Username | Date | Hashtags | Likes | Retweets | Url\n",
        "\n",
        "def create_tweets(tweets_json):\n",
        "    tweet_dict = defaultdict()\n",
        "    tweets = []\n",
        "\n",
        "    for key in tweets_json:\n",
        "        tweet_data = {\n",
        "            'id': tweets_json[key]['id'],\n",
        "            'full_text': tweets_json[key]['full_text'],\n",
        "            'tokens': build_terms(tweets_json[key]['full_text']),\n",
        "            'username': tweets_json[key]['user']['name'],\n",
        "            'date': tweets_json[key]['created_at'],\n",
        "            'hashtags': [key['text'] for key in tweets_json[key]['entities']['hashtags']],\n",
        "            'likes': tweets_json[key]['favorite_count'],\n",
        "            'retweets': tweets_json[key]['retweet_count'], \n",
        "        }\n",
        "\n",
        "        #sometimes the tweet url doesn't exist\n",
        "        try:\n",
        "            tweet_data['url'] = tweets_json[key]['entities']['media'][0]['url']\n",
        "        except:\n",
        "            tweet_data['url'] = None\n",
        "        \n",
        "        tweets.append(tweet_data)\n",
        "    return tweets"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# **CREATE INDEXES**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# create index\n",
        "def create_index(tweets_json):\n",
        "    tweets = create_tweets(tweets_json)\n",
        "    index = defaultdict(list)\n",
        "    title_index = defaultdict()\n",
        "\n",
        "    for tweet in tweets:\n",
        "        title_index[tweet['id']] = tweet\n",
        "        \n",
        "        #current page index keeps track of postision of each word in tweet\n",
        "        #e.x. if our tweet #50 has tokens \"covid health world covid\", our current_page_index looks like:\n",
        "        # {covid -> [50, [0, 3]], health -> [50, [1]], world [50, [2]]}\n",
        "        current_page_index = {}\n",
        "        for position, word in enumerate(tweet['tokens']):\n",
        "            \n",
        "            try:\n",
        "                # if the term is already in the index for the current page (current_page_index)\n",
        "                # append the position to the corresponding list\n",
        "                current_page_index[word][1].append(position)  \n",
        "            except:\n",
        "                # Add the new term as dict key and initialize the array of positions and add the position\n",
        "                current_page_index[word]=[tweet['id'], array('I', [position])] #'I' indicates unsigned int (int in Python)\n",
        "        \n",
        "\n",
        "        for term_page, posting_page in current_page_index.items():\n",
        "            index[term_page].append(posting_page)\n",
        "    \n",
        "    return index, title_index"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# apply tf-idf\n",
        "# tweets is a list of tokens\n",
        "def create_tfidf_index(tweets):\n",
        "    index = defaultdict(list)\n",
        "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
        "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
        "    title_index = defaultdict()\n",
        "    idf = defaultdict(float)\n",
        "\n",
        "\n",
        "    for tweet in tweets:\n",
        "        \n",
        "        title_index[tweet['id']] = tweet\n",
        "        current_page_index = {}\n",
        "\n",
        "        for position, term in enumerate(tweet['tokens']):  ## terms contains page_title + page_text\n",
        "            try:\n",
        "                # if the term is already in the dict append the position to the corresponding list\n",
        "                current_page_index[term][1].append(position)\n",
        "            except:\n",
        "                # Add the new term as dict key and initialize the array of positions and add the position\n",
        "                current_page_index[term]=[tweet['id'], array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
        "\n",
        "        #normalize term frequencies\n",
        "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
        "        # norm is the same for all terms of a document.\n",
        "        norm = 0\n",
        "        for term, posting in current_page_index.items():\n",
        "            # posting will contain the list of positions for current term in current document. \n",
        "            # posting ==> [current_doc, [list of positions]] \n",
        "            # you can use it to infer the frequency of current term.\n",
        "            \n",
        "            #CHECK THIS!\n",
        "            norm += len(posting[1]) ** 2\n",
        "        norm = math.sqrt(norm)\n",
        "\n",
        "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
        "        for term, posting in current_page_index.items():\n",
        "            # append the tf for current term (tf = term frequency in current doc\/norm)\n",
        "            tf[term].append(np.round(len(posting[1])\/norm, 4)) ## SEE formula (1) above\n",
        "            #increment the document frequency of current term (number of documents containing the current term)\n",
        "            df[term] += 1 # increment DF for current term\n",
        "\n",
        "        #merge the current page index with the main index\n",
        "        for term_page, posting_page in current_page_index.items():\n",
        "            index[term_page].append(posting_page)\n",
        "\n",
        "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
        "        for term in df:\n",
        "            idf[term] = 1 + np.round(np.log(float(len(tweets)\/df[term])), 4)\n",
        "\n",
        "    return index, tf, df, idf, title_index"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# **RANK TF-IDF + COS SIMILARITY; CUSTOM RANKING **"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def search_ranking(query, index, mode = 'TF-IDF'):\n",
        "    \"\"\"\n",
        "    output is the list of documents that contain any of the query terms. \n",
        "    So, we will get the list of documents for each query term, and take the union of them.\n",
        "    \"\"\"\n",
        "    query = build_terms(query)\n",
        "    docs = set()\n",
        "    for term in query:\n",
        "        try:\n",
        "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
        "            term_docs=[posting[0] for posting in index[term]]\n",
        "            \n",
        "            # docs = docs Union term_docs\n",
        "            docs = docs.union(set(term_docs))\n",
        "            #docs = set(term_docs)\n",
        "            #print(docs)\n",
        "        except:\n",
        "            #term is not in index\n",
        "            pass\n",
        "        \n",
        "\n",
        "    docs = list(docs)\n",
        "    if mode == 'TF-IDF':\n",
        "        ranked_docs, pred_score = rank_documents_tfidf_cos(query, docs, index, idf, tf, title_index)\n",
        "    elif mode == 'custom':\n",
        "        ranked_docs, pred_score =rank_documents_custom(query, docs, index, tf, title_index)\n",
        "    else:\n",
        "        ranked_docs, pred_score = rank_documents_bm25_custom(query, docs, index, tf, title_index)\n",
        "    return ranked_docs, pred_score"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def rank_documents_tfidf_cos(terms, docs, index, idf, tf, title_index):\n",
        "    \"\"\"\n",
        "    Perform the ranking of the results of a search based on the tf-idf weights\n",
        "    \n",
        "    Argument:\n",
        "    terms -- list of query terms\n",
        "    docs -- list of documents, to rank, matching the query\n",
        "    index -- inverted index data structure\n",
        "    idf -- inverted document frequencies\n",
        "    tf -- term frequencies\n",
        "    title_index -- mapping between page id and page title\n",
        "    \n",
        "    Returns:\n",
        "    Print the list of ranked documents\n",
        "    \"\"\"\n",
        "\n",
        "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
        "    # The remaining elements would became 0 when multiplied to the query_vector\n",
        "    # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
        "    doc_vectors = defaultdict(lambda: [0] * len(terms)) \n",
        "    query_vector = [0] * len(terms)\n",
        "\n",
        "    # compute the norm for the query tf\n",
        "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
        "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
        "    #HINT: use when computing tf for query_vector\n",
        "\n",
        "    query_norm = la.norm(list(query_terms_count.values()))\n",
        "\n",
        "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
        "        if term not in index:\n",
        "            continue\n",
        "\n",
        "        ## Compute tf*idf(normalize TF as done with documents)\n",
        "        query_vector[termIndex] = query_terms_count[term]\/query_norm * idf[term] \n",
        "\n",
        "        # Generate doc_vectors for matching docs\n",
        "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
        "            # Example of [doc_index, (doc, postings)]\n",
        "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
        "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
        "            # term is in doc 26 in positions 1,4, .....\n",
        "            # term is in doc 33 in positions 26,33, .....\n",
        "\n",
        "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
        "            if doc in docs:\n",
        "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  \n",
        "\n",
        "    # Calculate the score of each doc \n",
        "    # compute the cosine similarity between queyVector and each docVector:\n",
        "    \n",
        "    doc_scores=[[np.dot(curDocVec, query_vector) \/ (np.linalg.norm(curDocVec) * np.linalg.norm(query_vector)), doc] for doc, curDocVec in doc_vectors.items() ]\n",
        "    doc_scores.sort(reverse=True)\n",
        "    result_docs = [x[1] for x in doc_scores]\n",
        "    result_pred_score = [x[0] for x in doc_scores]\n",
        "\n",
        "    #print document titles instead if document id's\n",
        "    #result_docs=[ title_index[x] for x in result_docs ]\n",
        "    if len(result_docs) == 0:\n",
        "        print(\"No results found, try again\")\n",
        "        query = input()\n",
        "        docs = search_ranking(query, index)\n",
        "    #print ('\\n'.join(result_docs), '\\n')\n",
        "    return result_docs, result_pred_score"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def rank_documents_custom(terms, docs, index, tf, title_index):    \n",
        "\n",
        "    doc_vectors = defaultdict(lambda: [0] * len(terms)) \n",
        "    query_vector = [0] * len(terms)\n",
        "\n",
        "    query_terms_count = collections.Counter(terms)\n",
        "    #query_norm = la.norm(list(query_terms_count.values()))\n",
        "\n",
        "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
        "        if term not in index:\n",
        "            continue\n",
        "\n",
        "        ## Compute tf*idf(normalize TF as done with documents)\n",
        "        query_vector[termIndex] = query_terms_count[term] #\/ query_norm \n",
        "\n",
        "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
        "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
        "            if doc in docs:\n",
        "                doc_vectors[doc][termIndex] = tf[term][doc_index] \n",
        "    \n",
        "\n",
        "    # Calculate the score of each doc \n",
        "    # compute the cosine similarity between queyVector and each docVector\n",
        "    # weight the score based on likes and retweets\n",
        "\n",
        "    doc_scores=[[(np.dot(curDocVec, query_vector) \/ (np.linalg.norm(curDocVec) * np.linalg.norm(query_vector)))*0.7 + ((int(title_index[doc]['likes']) * 0.4) + int(title_index[doc]['retweets']) * 0.6)*0.3, doc] for doc, curDocVec in doc_vectors.items() ]\n",
        "    doc_scores.sort(reverse=True)\n",
        "    result_docs = [x[1] for x in doc_scores]\n",
        "    result_pred_score = [x[0] for x in doc_scores]\n",
        "\n",
        "\n",
        "\n",
        "    return result_docs, result_pred_score"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from rank_bm25 import BM25Okapi\n",
        "def rank_documents_bm25_custom(terms, docs, index, tf, title_index):\n",
        "    \"\"\"\n",
        "    we use external library to calculate the bm25\n",
        "    all algorithms are in this paper http:\/\/www.cs.otago.ac.nz\/homepages\/andrew\/papers\/2014-2.pdf\n",
        "    \n",
        "    This function, similar to the other ranking functions, calculates the custom bm25 score\n",
        "    \n",
        "    Argument:\n",
        "    terms -- list of query terms\n",
        "    docs -- list of documents, to rank, matching the query\n",
        "    index -- inverted index data structure\n",
        "    idf -- inverted document frequencies\n",
        "    tf -- term frequencies\n",
        "    title_index -- mapping between page id and page title\n",
        "    \n",
        "    Returns:\n",
        "    Print the list of ranked documents\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    try:\n",
        "        tokenized_corpus = [title_index[doc_id]['tokens'] for doc_id in docs]\n",
        "    except:\n",
        "        print(docs)\n",
        "    bm25_model = BM25Okapi(tokenized_corpus)\n",
        "    \n",
        "    #bm25_score calculates the bm25 score for each documents, given the query vector 'terms'\n",
        "    bm25_score = bm25_model.get_scores(terms)\n",
        "    \n",
        "    #we will now calculate our custom score\n",
        "    updated_results = []\n",
        "    for i in range(len(docs)):\n",
        "        curr_bm25 = bm25_score[i]\n",
        "        tweet = docs[i]\n",
        "        \n",
        "        #initialize the metrics that we will use to calculate custom score\n",
        "        #explanations and motivations are in the writeup\n",
        "        \n",
        "        #curr_length_hashtag_ratio is 1 + log(len(tweet)\/(num of hashtags))\n",
        "        curr_length_hashtag_ratio = 1\n",
        "        \n",
        "        #curr_num_likes is 1 + log(len(likes))\n",
        "        curr_num_likes = 1\n",
        "        \n",
        "        #curr_num_retweets is 1 + log(len(retweets))\n",
        "        curr_retweets = 1\n",
        "        \n",
        "        #multiple try clauses in case we divide by 0 or take the logarithm of 0\n",
        "        try:\n",
        "            curr_length_hashtag_ratio = 1 + np.log(len(tweet['tokens'])\/len(tweet['hashtags']))\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "            curr_num_likes = 1 + np.log(int(tweet['likes']))\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        try:\n",
        "            curr_retweets = 1 + np.log(int(tweet['retweets']))\n",
        "            score = curr_bm25 * curr_length_hashtag_ratio * curr_num_likes * curr_retweets\n",
        "            updated_results.append(score)\n",
        "        except:\n",
        "            pass\n",
        "        return docs, updated_results\n",
        "    "
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "tweets = create_tweets(tweets_json)\n",
        "index, tf, df, idf, title_index = create_tfidf_index(tweets)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
        "query = input()\n",
        "#play around with this function! the third argument is either 'TF-IDF', 'custom', or 'bm25'\n",
        "ranked_docs, _ = search_ranking(query, index, 'bm25')\n",
        "\n",
        "top = 20\n",
        "\n",
        "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
        "count = 1\n",
        "#print(ranked_docs[0])\n",
        "for d_id in ranked_docs[:top]:\n",
        "    print(\"rank: {}\".format(count))\n",
        "    #print(d_id)\n",
        "    #print(title_index)\n",
        "    pretty_print_tweet(title_index[d_id])\n",
        "    count += 1"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# WORD2VEC"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "###word2vec\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "import nltk\n",
        "#create a word2Vec model\n",
        "words = [tweet['tokens'] for tweet in tweets]\n",
        "w2v_model = Word2Vec(sentences = words, size = 100, min_count = 1, window = 10, negative = 15, sg = 0)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#check size of our w2vector\n",
        "w2v_model.vector_size"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def avg_vector(model, terms):\n",
        "    \"\"\"\n",
        "    this function averages the vectors that are the word embeddings for a series of terms\n",
        "    \n",
        "    Argument:\n",
        "    model -- our trained w2v model\n",
        "    terms -- list of tokenized words e.x. ['covid', 'reduct', 'vacc']\n",
        "    \n",
        "    Returns:\n",
        "    np ndarray of the \"average\" word embedding of our terms\n",
        "    \"\"\"\n",
        "    \n",
        "    assert(len(terms) > 0)\n",
        "    result = np.ndarray(shape = (model.vector_size,))\n",
        "  \n",
        "    \n",
        "    for term in terms:\n",
        "        try:\n",
        "            curr_vec = model[term]\n",
        "            result = np.add(curr_vec, result)\n",
        "        except:\n",
        "            print(term + \" not found\")\n",
        "    return result\/len(terms)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Now we need to create a w2v representation of each tweet\n",
        "# output is a dicionary mapping tweet ID to their vector representations\n",
        "def create_tweet_vecs(model, title_index):\n",
        "    \"\"\" \n",
        "    Argument:\n",
        "    model -- our trained w2v model\n",
        "    title_index -- mapping between tweet id and tweet contents\n",
        "    \n",
        "    Returns:\n",
        "    Dictionary that maps tweet IDs to their vector representations\"\"\"\n",
        "    result = dict()\n",
        "    for key in title_index.keys():\n",
        "        tokens = title_index[key]['tokens']\n",
        "        if len(tokens) > 0:\n",
        "            tweet_vec = avg_vector(model, tokens)\n",
        "            result[key] = tweet_vec\n",
        "    return result\n",
        "tweet_vecs = create_tweet_vecs(w2v_model, title_index)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# compute cosine similarity\n",
        "def cos_similarity(query, doc):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    query -- w2v representation of query terms\n",
        "    doc -- w2v representation of terms in the tweet\n",
        "\n",
        "    \n",
        "    Returns:\n",
        "    cosine similarity of the two vectors\n",
        "    \"\"\"\n",
        "    return np.dot(query, doc) \/ (np.linalg.norm(query) * np.linalg.norm(doc))\n",
        "\n",
        "\n",
        "# function that prints our top k results, using word2vec and cosine similarity\n",
        "def rank_top_k(query_vec, tweet_vecs, title_index, k = 20):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        query_vec -- w2v representation of our query\n",
        "        tweet_vecs -- dictionary that maps tweet ids to their vector representations\n",
        "        title_index -- maps tweet ids to their contents\n",
        "        k -- parameter to control how many tweets to print out\n",
        "    \"\"\"\n",
        "    \n",
        "    #result maps Tweet ids to their cosine similarity scores with the query\n",
        "    result = dict()\n",
        "    \n",
        "    for key in tweet_vecs.keys():\n",
        "        result[key] = cos_similarity(query_vec, tweet_vecs[key])\n",
        "    result = sorted(result.items() ,key=lambda item: item[1], reverse=True)[:k]\n",
        "    \n",
        "    rank = 1\n",
        "    for (id, score) in result:\n",
        "        print(rank, \" score: \", score)\n",
        "        pretty_print_tweet(title_index[id])\n",
        "        rank += 1"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "queries = [\"covid\", \"vaccine\", \"global health\", \"end pandemic\", \"death risk\"]\n",
        "queries = [build_terms(query) for query in queries]\n",
        "\n",
        "for query in queries:\n",
        "    print(query)\n",
        "    query_vec = avg_vector(w2v_model, query)\n",
        "    results = rank_top_k(query_vec, tweet_vecs, title_index)\n",
        "    print('------------------------')\n",
        "    "
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}