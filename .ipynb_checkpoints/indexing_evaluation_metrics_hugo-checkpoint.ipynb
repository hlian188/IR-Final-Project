{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset_tweets_WHO.txt'\n",
    "\n",
    "#convert the text to json\n",
    "with open(path) as f:\n",
    "    tweets_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"contributors\": null,\n",
      "    \"coordinates\": null,\n",
      "    \"created_at\": \"Mon Oct 11 04:43:20 +0000 2021\",\n",
      "    \"display_text_range\": [\n",
      "        0,\n",
      "        140\n",
      "    ],\n",
      "    \"entities\": {\n",
      "        \"hashtags\": [],\n",
      "        \"symbols\": [],\n",
      "        \"urls\": [],\n",
      "        \"user_mentions\": [\n",
      "            {\n",
      "                \"id\": 3794682452,\n",
      "                \"id_str\": \"3794682452\",\n",
      "                \"indices\": [\n",
      "                    3,\n",
      "                    11\n",
      "                ],\n",
      "                \"name\": \"World Health Organization (WHO) Western Pacific\",\n",
      "                \"screen_name\": \"WHOWPRO\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"favorite_count\": 0,\n",
      "    \"favorited\": false,\n",
      "    \"full_text\": \"RT @WHOWPRO: \\u201cMy patients are no different to my grandmother and grandfather.\\u201d \\n\\nLoyal to their oath, health workers like Dr Gantsengel Pur\\u2026\",\n",
      "    \"geo\": null,\n",
      "    \"id\": 1447422540335484932,\n",
      "    \"id_str\": \"1447422540335484932\",\n",
      "    \"in_reply_to_screen_name\": null,\n",
      "    \"in_reply_to_status_id\": null,\n",
      "    \"in_reply_to_status_id_str\": null,\n",
      "    \"in_reply_to_user_id\": null,\n",
      "    \"in_reply_to_user_id_str\": null,\n",
      "    \"is_quote_status\": false,\n",
      "    \"lang\": \"en\",\n",
      "    \"place\": null,\n",
      "    \"retweet_count\": 30,\n",
      "    \"retweeted\": false,\n",
      "    \"retweeted_status\": {\n",
      "        \"contributors\": null,\n",
      "        \"coordinates\": null,\n",
      "        \"created_at\": \"Sun Oct 10 14:00:00 +0000 2021\",\n",
      "        \"display_text_range\": [\n",
      "            0,\n",
      "            274\n",
      "        ],\n",
      "        \"entities\": {\n",
      "            \"hashtags\": [\n",
      "                {\n",
      "                    \"indices\": [\n",
      "                        195,\n",
      "                        203\n",
      "                    ],\n",
      "                    \"text\": \"COVID19\"\n",
      "                },\n",
      "                {\n",
      "                    \"indices\": [\n",
      "                        260,\n",
      "                        273\n",
      "                    ],\n",
      "                    \"text\": \"MentalHealth\"\n",
      "                }\n",
      "            ],\n",
      "            \"media\": [\n",
      "                {\n",
      "                    \"display_url\": \"pic.twitter.com/LCUgSGx0u8\",\n",
      "                    \"expanded_url\": \"https://twitter.com/WHOWPRO/status/1447200245549514758/video/1\",\n",
      "                    \"id\": 1396722733136846849,\n",
      "                    \"id_str\": \"1396722733136846849\",\n",
      "                    \"indices\": [\n",
      "                        275,\n",
      "                        298\n",
      "                    ],\n",
      "                    \"media_url\": \"http://pbs.twimg.com/media/E2IqcGcUcAQhwnN.jpg\",\n",
      "                    \"media_url_https\": \"https://pbs.twimg.com/media/E2IqcGcUcAQhwnN.jpg\",\n",
      "                    \"sizes\": {\n",
      "                        \"large\": {\n",
      "                            \"h\": 270,\n",
      "                            \"resize\": \"fit\",\n",
      "                            \"w\": 480\n",
      "                        },\n",
      "                        \"medium\": {\n",
      "                            \"h\": 270,\n",
      "                            \"resize\": \"fit\",\n",
      "                            \"w\": 480\n",
      "                        },\n",
      "                        \"small\": {\n",
      "                            \"h\": 270,\n",
      "                            \"resize\": \"fit\",\n",
      "                            \"w\": 480\n",
      "                        },\n",
      "                        \"thumb\": {\n",
      "                            \"h\": 150,\n",
      "                            \"resize\": \"crop\",\n",
      "                            \"w\": 150\n",
      "                        }\n",
      "                    },\n",
      "                    \"type\": \"photo\",\n",
      "                    \"url\": \"https://t.co/LCUgSGx0u8\"\n",
      "                }\n",
      "            ],\n",
      "            \"symbols\": [],\n",
      "            \"urls\": [],\n",
      "            \"user_mentions\": []\n",
      "        },\n",
      "        \"extended_entities\": {\n",
      "            \"media\": [\n",
      "                {\n",
      "                    \"additional_media_info\": {\n",
      "                        \"call_to_actions\": {\n",
      "                            \"visit_site\": {\n",
      "                                \"url\": \"https://www.who.int/mongolia\"\n",
      "                            }\n",
      "                        },\n",
      "                        \"description\": \"\",\n",
      "                        \"embeddable\": true,\n",
      "                        \"monetizable\": false,\n",
      "                        \"title\": \"Let us #SupportHealthCareWorkers on #WorldMentalHealthDay.\"\n",
      "                    },\n",
      "                    \"display_url\": \"pic.twitter.com/LCUgSGx0u8\",\n",
      "                    \"expanded_url\": \"https://twitter.com/WHOWPRO/status/1447200245549514758/video/1\",\n",
      "                    \"id\": 1396722733136846849,\n",
      "                    \"id_str\": \"1396722733136846849\",\n",
      "                    \"indices\": [\n",
      "                        275,\n",
      "                        298\n",
      "                    ],\n",
      "                    \"media_url\": \"http://pbs.twimg.com/media/E2IqcGcUcAQhwnN.jpg\",\n",
      "                    \"media_url_https\": \"https://pbs.twimg.com/media/E2IqcGcUcAQhwnN.jpg\",\n",
      "                    \"sizes\": {\n",
      "                        \"large\": {\n",
      "                            \"h\": 270,\n",
      "                            \"resize\": \"fit\",\n",
      "                            \"w\": 480\n",
      "                        },\n",
      "                        \"medium\": {\n",
      "                            \"h\": 270,\n",
      "                            \"resize\": \"fit\",\n",
      "                            \"w\": 480\n",
      "                        },\n",
      "                        \"small\": {\n",
      "                            \"h\": 270,\n",
      "                            \"resize\": \"fit\",\n",
      "                            \"w\": 480\n",
      "                        },\n",
      "                        \"thumb\": {\n",
      "                            \"h\": 150,\n",
      "                            \"resize\": \"crop\",\n",
      "                            \"w\": 150\n",
      "                        }\n",
      "                    },\n",
      "                    \"type\": \"video\",\n",
      "                    \"url\": \"https://t.co/LCUgSGx0u8\",\n",
      "                    \"video_info\": {\n",
      "                        \"aspect_ratio\": [\n",
      "                            16,\n",
      "                            9\n",
      "                        ],\n",
      "                        \"duration_millis\": 175000,\n",
      "                        \"variants\": [\n",
      "                            {\n",
      "                                \"bitrate\": 2176000,\n",
      "                                \"content_type\": \"video/mp4\",\n",
      "                                \"url\": \"https://video.twimg.com/amplify_video/1396722733136846849/vid/1280x720/RJvJVJZbErkaPtev.mp4?tag=14\"\n",
      "                            },\n",
      "                            {\n",
      "                                \"content_type\": \"application/x-mpegURL\",\n",
      "                                \"url\": \"https://video.twimg.com/amplify_video/1396722733136846849/pl/davAgaTXvo8mSYRa.m3u8?tag=14\"\n",
      "                            },\n",
      "                            {\n",
      "                                \"bitrate\": 832000,\n",
      "                                \"content_type\": \"video/mp4\",\n",
      "                                \"url\": \"https://video.twimg.com/amplify_video/1396722733136846849/vid/640x360/ABczbVfdO0SUm8At.mp4?tag=14\"\n",
      "                            },\n",
      "                            {\n",
      "                                \"bitrate\": 288000,\n",
      "                                \"content_type\": \"video/mp4\",\n",
      "                                \"url\": \"https://video.twimg.com/amplify_video/1396722733136846849/vid/480x270/CtFCaEeSa2TPEmH-.mp4?tag=14\"\n",
      "                            }\n",
      "                        ]\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"favorite_count\": 97,\n",
      "        \"favorited\": false,\n",
      "        \"full_text\": \"\\u201cMy patients are no different to my grandmother and grandfather.\\u201d \\n\\nLoyal to their oath, health workers like Dr Gantsengel Purev of \\ud83c\\uddf2\\ud83c\\uddf3 have worked day and night to protect and save patients from #COVID19 despite their own struggles and the impact on their own #MentalHealth. https://t.co/LCUgSGx0u8\",\n",
      "        \"geo\": null,\n",
      "        \"id\": 1447200245549514758,\n",
      "        \"id_str\": \"1447200245549514758\",\n",
      "        \"in_reply_to_screen_name\": null,\n",
      "        \"in_reply_to_status_id\": null,\n",
      "        \"in_reply_to_status_id_str\": null,\n",
      "        \"in_reply_to_user_id\": null,\n",
      "        \"in_reply_to_user_id_str\": null,\n",
      "        \"is_quote_status\": false,\n",
      "        \"lang\": \"en\",\n",
      "        \"place\": null,\n",
      "        \"possibly_sensitive\": false,\n",
      "        \"retweet_count\": 30,\n",
      "        \"retweeted\": false,\n",
      "        \"source\": \"<a href=\\\"https://studio.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Media Studio</a>\",\n",
      "        \"truncated\": false,\n",
      "        \"user\": {\n",
      "            \"contributors_enabled\": false,\n",
      "            \"created_at\": \"Mon Oct 05 18:08:31 +0000 2015\",\n",
      "            \"default_profile\": false,\n",
      "            \"default_profile_image\": false,\n",
      "            \"description\": \"World Health Organization @WHO in the Western #Pacific Region, \\ud83c\\udfe0 to 1.9B in #Asia & #Oceania. See new tweets for updated #COVID19 advice. \\u27a1\\ufe0f RD @takeshi_kasai\",\n",
      "            \"entities\": {\n",
      "                \"description\": {\n",
      "                    \"urls\": []\n",
      "                },\n",
      "                \"url\": {\n",
      "                    \"urls\": [\n",
      "                        {\n",
      "                            \"display_url\": \"who.int/westernpacific\",\n",
      "                            \"expanded_url\": \"http://www.who.int/westernpacific\",\n",
      "                            \"indices\": [\n",
      "                                0,\n",
      "                                23\n",
      "                            ],\n",
      "                            \"url\": \"https://t.co/EblVamM8WW\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            \"favourites_count\": 6152,\n",
      "            \"follow_request_sent\": false,\n",
      "            \"followers_count\": 219413,\n",
      "            \"following\": false,\n",
      "            \"friends_count\": 1229,\n",
      "            \"geo_enabled\": true,\n",
      "            \"has_extended_profile\": true,\n",
      "            \"id\": 3794682452,\n",
      "            \"id_str\": \"3794682452\",\n",
      "            \"is_translation_enabled\": false,\n",
      "            \"is_translator\": false,\n",
      "            \"lang\": null,\n",
      "            \"listed_count\": 668,\n",
      "            \"location\": \"Manila, Philippines\",\n",
      "            \"name\": \"World Health Organization (WHO) Western Pacific\",\n",
      "            \"notifications\": false,\n",
      "            \"profile_background_color\": \"000000\",\n",
      "            \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "            \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "            \"profile_background_tile\": false,\n",
      "            \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/3794682452/1616740141\",\n",
      "            \"profile_image_url\": \"http://pbs.twimg.com/profile_images/899826905532125184/yPM5mxkX_normal.jpg\",\n",
      "            \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/899826905532125184/yPM5mxkX_normal.jpg\",\n",
      "            \"profile_link_color\": \"3B94D9\",\n",
      "            \"profile_sidebar_border_color\": \"000000\",\n",
      "            \"profile_sidebar_fill_color\": \"000000\",\n",
      "            \"profile_text_color\": \"000000\",\n",
      "            \"profile_use_background_image\": false,\n",
      "            \"protected\": false,\n",
      "            \"screen_name\": \"WHOWPRO\",\n",
      "            \"statuses_count\": 10596,\n",
      "            \"time_zone\": null,\n",
      "            \"translator_type\": \"none\",\n",
      "            \"url\": \"https://t.co/EblVamM8WW\",\n",
      "            \"utc_offset\": null,\n",
      "            \"verified\": true,\n",
      "            \"withheld_in_countries\": []\n",
      "        }\n",
      "    },\n",
      "    \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\",\n",
      "    \"truncated\": false,\n",
      "    \"user\": {\n",
      "        \"contributors_enabled\": false,\n",
      "        \"created_at\": \"Wed Apr 23 19:56:27 +0000 2008\",\n",
      "        \"default_profile\": false,\n",
      "        \"default_profile_image\": false,\n",
      "        \"description\": \"We are the #UnitedNations\\u2019 health agency - #HealthForAll.\\n\\u25b6\\ufe0f Always check our latest tweets on #COVID19 for updated advice/information.\",\n",
      "        \"entities\": {\n",
      "            \"description\": {\n",
      "                \"urls\": []\n",
      "            },\n",
      "            \"url\": {\n",
      "                \"urls\": [\n",
      "                    {\n",
      "                        \"display_url\": \"who.int\",\n",
      "                        \"expanded_url\": \"http://www.who.int\",\n",
      "                        \"indices\": [\n",
      "                            0,\n",
      "                            23\n",
      "                        ],\n",
      "                        \"url\": \"https://t.co/wVulKuROWG\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"favourites_count\": 11879,\n",
      "        \"follow_request_sent\": false,\n",
      "        \"followers_count\": 9963586,\n",
      "        \"following\": false,\n",
      "        \"friends_count\": 1743,\n",
      "        \"geo_enabled\": true,\n",
      "        \"has_extended_profile\": true,\n",
      "        \"id\": 14499829,\n",
      "        \"id_str\": \"14499829\",\n",
      "        \"is_translation_enabled\": false,\n",
      "        \"is_translator\": false,\n",
      "        \"lang\": null,\n",
      "        \"listed_count\": 34215,\n",
      "        \"location\": \"Geneva, Switzerland\",\n",
      "        \"name\": \"World Health Organization (WHO)\",\n",
      "        \"notifications\": false,\n",
      "        \"profile_background_color\": \"D0ECF8\",\n",
      "        \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "        \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\",\n",
      "        \"profile_background_tile\": true,\n",
      "        \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/14499829/1610970935\",\n",
      "        \"profile_image_url\": \"http://pbs.twimg.com/profile_images/875476478988886016/_l61qZdR_normal.jpg\",\n",
      "        \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/875476478988886016/_l61qZdR_normal.jpg\",\n",
      "        \"profile_link_color\": \"0396DB\",\n",
      "        \"profile_sidebar_border_color\": \"8C8C8C\",\n",
      "        \"profile_sidebar_fill_color\": \"D9D9D9\",\n",
      "        \"profile_text_color\": \"000000\",\n",
      "        \"profile_use_background_image\": true,\n",
      "        \"protected\": false,\n",
      "        \"screen_name\": \"WHO\",\n",
      "        \"statuses_count\": 64983,\n",
      "        \"time_zone\": null,\n",
      "        \"translator_type\": \"regular\",\n",
      "        \"url\": \"https://t.co/wVulKuROWG\",\n",
      "        \"utc_offset\": null,\n",
      "        \"verified\": true,\n",
      "        \"withheld_in_countries\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(tweets_json['50'], indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(line):\n",
    "    \"\"\"\n",
    "    Helper function to remove punctuation EXCEPT for '#''\n",
    "    \n",
    "    Arugment:\n",
    "    line -- string of text\n",
    "    \n",
    "    Returns:\n",
    "    line -- string of text without punctuation\n",
    "    \"\"\"\n",
    "    return line.translate(str.maketrans('', '', string.punctuation.replace('#', '')))\n",
    "\n",
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the Tweet text by removing stop words, emojis, and punctuation and\n",
    "    stemming, transforming to lowercase and returning the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line -- a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # transform to lowercase \n",
    "    line =  line.lower() \n",
    "    \n",
    "    # remove non-ASCII terms like emojis and symbols\n",
    "    line = \"\".join(c for c in line if c in string.printable) \n",
    "    \n",
    "    # remove punctuation EXCEPT for hashtags (see remove_punct())\n",
    "    line = remove_punct(line)\n",
    "    \n",
    "    # tokenize the text to get a list of terms\n",
    "    line = line.split() \n",
    "    \n",
    "    # remove html tags, blank spaces like '', and urls\n",
    "    line = [word for word in line if not (re.match(\"^qampa$\" , word) or re.match(\"^amp$\" , word) or re.match(\"^http\" , word)) \n",
    "    and word] \n",
    "    \n",
    "    # remove standalone numbers e.x. '19' but not the 19 from 'covid19'\n",
    "    line = [word for word in line if not word.isnumeric()]\n",
    "    \n",
    "    # add standalone word as token too if it has number e.x. 'covid19' gets tokenized as 'covid19' and 'covid'\n",
    "    line = line + [word.rstrip(string.digits) for word in line if sum([c.isdigit() for c in word]) != 0]\n",
    "    \n",
    "    # remove stopwords\n",
    "    line = [word for word in line if word not in stop_words] \n",
    "    \n",
    "    # perform stemming\n",
    "    line = [stemmer.stem(word) for word in line]\n",
    "    \n",
    "    # add unhashtagged word if it's hashtag is present \n",
    "    # e.x. if #covid is present, we also add covid as a token\n",
    "    line = line + [word.replace('#', '') for word in line if word[0] == '#' ] \n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_dict is our output data structure that maps Tweet IDs to their text\n",
    "# note we need to keep the following information\n",
    "# Tweet | Username | Date | Hashtags | Likes | Retweets | Url\n",
    "\n",
    "def create_tweets(tweets_json):\n",
    "    tweet_dict = defaultdict()\n",
    "    tweets = []\n",
    "\n",
    "    for key in tweets_json:\n",
    "        tweet_data = {\n",
    "            'id': tweets_json[key]['id'],\n",
    "            'full_text': tweets_json[key]['full_text'],\n",
    "            'tokens': build_terms(tweets_json[key]['full_text']),\n",
    "            'username': tweets_json[key]['user']['name'],\n",
    "            'date': tweets_json[key]['created_at'],\n",
    "            'hashtags': [key['text'] for key in tweets_json[key]['entities']['hashtags']],\n",
    "            'likes': tweets_json[key]['favorite_count'],\n",
    "            'retweets': tweets_json[key]['retweet_count'], \n",
    "        }\n",
    "\n",
    "        #sometimes the tweet url doesn't exist\n",
    "        try:\n",
    "            tweet_data['url'] = tweets_json[key]['entities']['media'][0]['url']\n",
    "        except:\n",
    "            tweet_data['url'] = None\n",
    "        \n",
    "        tweets.append(tweet_data)\n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "def create_index(tweets_json):\n",
    "    tweets = create_tweets(tweets_json)\n",
    "    index = defaultdict(list)\n",
    "    title_index = defaultdict()\n",
    "\n",
    "    for tweet in tweets:\n",
    "        title_index[tweet['id']] = tweet\n",
    "        \n",
    "        #current page index keeps track of postision of each word in tweet\n",
    "        #e.x. if our tweet #50 has tokens \"covid health world covid\", our current_page_index looks like:\n",
    "        # {covid -> [50, [0, 3]], health -> [50, [1]], world [50, [2]]}\n",
    "        current_page_index = {}\n",
    "        for position, word in enumerate(tweet['tokens']):\n",
    "            \n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                current_page_index[word][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[word]=[tweet['id'], array('I', [position])] #'I' indicates unsigned int (int in Python)\n",
    "        \n",
    "\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "    \n",
    "    return index, title_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'covid': [[50, array('I', [0, 3])], [60, array('I', [0])], [2, array('I', [0])]], 'health': [[50, array('I', [1])], [2, array('I', [1])]], 'world': [[50, array('I', [2])], [60, array('I', [3])]], 'medicine': [[60, array('I', [1])]], 'dog': [[60, array('I', [2])], [2, array('I', [2])]], 'ugh': [[2, array('I', [3])]], 'huh': [[2, array('I', [4])]]})\n",
      "defaultdict(<class 'list'>, {'covid': [0.8165, 0.5, 0.4472], 'health': [0.4082, 0.4472], 'world': [0.4082, 0.5], 'medicine': [0.5], 'dog': [0.5, 0.4472], 'ugh': [0.4472], 'huh': [0.4472]})\n",
      "defaultdict(<class 'int'>, {'covid': 3, 'health': 2, 'world': 2, 'medicine': 1, 'dog': 2, 'ugh': 1, 'huh': 1})\n",
      "defaultdict(<class 'float'>, {'covid': 1.0, 'health': 1.4055, 'world': 1.4055, 'medicine': 2.0986000000000002, 'dog': 1.4055, 'ugh': 2.0986000000000002, 'huh': 2.0986000000000002})\n",
      "defaultdict(None, {50: {'tokens': ['covid', 'health', 'world', 'covid'], 'id': 50}, 60: {'tokens': ['covid', 'medicine', 'dog', 'world'], 'id': 60}, 2: {'tokens': ['covid', 'health', 'dog', 'ugh', 'huh'], 'id': 2}})\n"
     ]
    }
   ],
   "source": [
    "# apply tf-idf\n",
    "# tweets is a list of tokens\n",
    "def create_tfidf_index(tweets):\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
    "    title_index = defaultdict()\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "\n",
    "    for tweet in tweets:\n",
    "        \n",
    "        title_index[tweet['id']] = tweet\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(tweet['tokens']):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweet['id'], array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            \n",
    "            #CHECK THIS!\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm, 4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = 1 + np.round(np.log(float(len(tweets)/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, title_index\n",
    "\n",
    "def test():\n",
    "    tweet1 = {'tokens' : ['covid', 'health', 'world', 'covid'], 'id' : 50}\n",
    "    tweet2 = {'tokens' : ['covid', 'medicine', 'dog', 'world'], 'id' : 60}\n",
    "    tweet3 = {'tokens' : ['covid', 'health', 'dog', 'ugh', 'huh'], 'id' : 2}\n",
    "    tweets = [tweet1, tweet2, tweet3]\n",
    "    index, tf, df, idf, title_index = create_tfidf_index(tweets)\n",
    "    print(index)\n",
    "    print(tf)\n",
    "    print(df)\n",
    "    print(idf)\n",
    "    print(title_index)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) \n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex] = query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    result_pred_score = [x[0] for x in doc_scores]\n",
    "\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs, result_pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(set(term_docs))\n",
    "            #docs = set(term_docs)\n",
    "            #print(docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "        \n",
    "\n",
    "    docs = list(docs)\n",
    "    ranked_docs, pred_score = rank_documents(query, docs, index, idf, tf, title_index)\n",
    "    return ranked_docs, pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tweets = create_tweets(tweets_json)\n",
    "start_time = time.time()\n",
    "index, tf, df, idf, title_index = create_tfidf_index(tweets)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_tweet(tweet):\n",
    "    print(\n",
    "    \"\"\"id: {}\n",
    "    username: {}\n",
    "    text: {}\n",
    "    date: {}\n",
    "    hashtags: {}\n",
    "    likes: {}\n",
    "    retweets: {}\n",
    "    url: {}\\n\"\"\".format(tweet['id'], tweet['username'], tweet['full_text'], tweet['date'], tweet['hashtags'], tweet['likes'],\n",
    "                        tweet['retweets'], tweet['url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs, _ = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "count = 1\n",
    "#print(ranked_docs[0])\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(\"rank: {}\".format(count))\n",
    "    #print(d_id)\n",
    "    #print(title_index)\n",
    "    pretty_print_tweet(title_index[d_id])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll print terms with highest df, tf, and idf\n",
    "\n",
    "#print(sorted(df.items(), key=lambda x: x[1], reverse=True)[:500], \"\\n\")\n",
    "#print(sorted(tf.items(), key=lambda x: x[1], reverse=True)[:50], \"\\n\")\n",
    "#print(sorted(idf.items(), key=lambda x: x[1], reverse=True)[:50], \"\\n\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We choose : “covid”, “vaccine”, “global health”, “end pandemic” and “death risk”***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# query 0 for example\n",
    "queries = ['covid','vaccine','global health','end pandemic','death risk']\n",
    "\n",
    "random.seed(101)\n",
    "\n",
    "# We create a DataFrame with the results of the seacrh (doc_id and pred_rel of the ranked docs)\n",
    "rows = []\n",
    "for query in queries:\n",
    "    doc_ids, scores = search_tf_idf(query, index)\n",
    "    for i in range(len(scores)):\n",
    "         rows.append([queries.index(query), doc_ids[i], scores[i], random.randint(0, 1)])\n",
    "    \n",
    "df = pd.DataFrame(rows ,columns=[\"q_id\", \"doc_id\", \"predicted_relevance\", \"y_true\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_score, k=50):\n",
    "    '''    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "    \n",
    "    '''    \n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    relevant = sum(y_true == 1)\n",
    "    return relevant / k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(y_true, y_score, k=50):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    '''\n",
    "    \n",
    "    gtp = np.sum(y_true == 1) \n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])           \n",
    "\n",
    "    ## if all docs are not relevant\n",
    "    if gtp==0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            n_relevant_at_i += 1 \n",
    "            prec_at_i += n_relevant_at_i / (i + 1) \n",
    "    return prec_at_i / gtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(y_true, y_score, k=50):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    '''\n",
    "\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true, order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    if np.sum(y_true) == 0: # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1 / (np.argmax(y_true == 1) + 1) # hint: to get the position of the first relevant document use \"np.argmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(y_true, y_score,  k=50):\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true, order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = (2 ** y_true) - 1 # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2) # Compute denominator\n",
    "    return np.sum(gain / discounts) #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(y_true, y_score, k=50): \n",
    "    order = np.argsort(y_true)[::-1]\n",
    "    ideal = np.take(y_true, order[:k])\n",
    "    dcg_max = dcg_at_k(ideal, ideal, k) # Ideal dcg\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(y_true, y_score, k)/dcg_max, 4)  # return ndcg@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df[df['q_id']==0]\n",
    "df1 = df[df['q_id']==1]\n",
    "df1.reset_index(inplace = True)\n",
    "df2 = df[df['q_id']==2]\n",
    "df2.reset_index(inplace = True)\n",
    "df3 = df[df['q_id']==3]\n",
    "df3.reset_index(inplace = True)\n",
    "df4 = df[df['q_id']==4]\n",
    "df4.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision at k = 10 for query 'covid' : \" + str(precision_at_k(df0['y_true'], df0['predicted_relevance'])))\n",
    "print(\"Average precision at k = 10 for query 'covid' : \" + str(avg_precision_at_k(df0['y_true'], df0['predicted_relevance'])))\n",
    "print(\"NDCG at k = 10 for query 'covid' : \" +str(ndcg_at_k(df0['y_true'],df0['predicted_relevance'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision at k = 10 for query 'vaccine' : \" + str(precision_at_k(df1['y_true'], df1['predicted_relevance'])))\n",
    "print(\"Average precision at k = 10 for query 'vaccine' : \" + str(avg_precision_at_k(df1['y_true'], df1['predicted_relevance'])))\n",
    "print(\"NDCG at k = 10 for query 'vaccine' : \" +str(ndcg_at_k(df1['y_true'],df1['predicted_relevance'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision at k = 10 for query 'global health' : \" + str(precision_at_k(df2['y_true'], df2['predicted_relevance'])))\n",
    "print(\"Average precision at k = 10 for query 'global health' : \" + str(avg_precision_at_k(df2['y_true'], df2['predicted_relevance'])))\n",
    "print(\"NDCG at k = 10 for query 'global health' : \" +str(ndcg_at_k(df2['y_true'],df2['predicted_relevance'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision at k = 10 for query 'end pandemic' : \" + str(precision_at_k(df3['y_true'], df3['predicted_relevance'])))\n",
    "print(\"Average precision at k = 10 for query 'end pandemic' : \" + str(avg_precision_at_k(df3['y_true'], df3['predicted_relevance'])))\n",
    "print(\"NDCG at k = 10 for query 'end pandemic' : \" +str(ndcg_at_k(df3['y_true'],df3['predicted_relevance'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision at k = 10 for query 'death risk' : \" + str(precision_at_k(df4['y_true'], df4['predicted_relevance'])))\n",
    "print(\"Average precision at k = 10 for query 'death risk' : \" + str(avg_precision_at_k(df4['y_true'], df4['predicted_relevance'])))\n",
    "print(\"NDCG at k = 10 for query 'death risk' : \" +str(ndcg_at_k(df4['y_true'],df4['predicted_relevance'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RRs = []\n",
    "for q in df['q_id'].unique(): # loop over all query ids\n",
    "    labels = np.array(df[df['q_id']==q]['y_true']) # get labels for current query\n",
    "    scores = np.array(df[df['q_id']==q]['predicted_relevance']) # get predicted score for current query\n",
    "    RRs.append(rr_at_k(labels,scores)) # append RR for current query\n",
    "mrr = np.round(float(sum(RRs)/len(RRs)),4) # Mean RR at current k\n",
    "print(\"MRR for the queries is : \" +str(mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_at_k = [avg_precision_at_k(df0['y_true'], df0['predicted_relevance']),avg_precision_at_k(df1['y_true'], df1['predicted_relevance']),avg_precision_at_k(df2['y_true'], df2['predicted_relevance']),avg_precision_at_k(df3['y_true'], df3['predicted_relevance']),avg_precision_at_k(df4['y_true'], df4['predicted_relevance'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = sum(av_at_k)/len(av_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAP for the 5 queries is : \" + str(MAP))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
